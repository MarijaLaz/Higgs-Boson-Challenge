{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip csv \n",
    "import zipfile\n",
    "with zipfile.ZipFile('../data/train.csv.zip') as zip_ref:\n",
    "    zip_ref.extractall(r\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "#load data\n",
    "DATA_TRAIN_PATH = '../data/train.csv' #download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "print(y.shape, x.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tx = x.copy()\\nfor i in range(x.shape[1]):\\n    idx = x[:,i] > -999\\n    mean = np.mean(x[idx,i])\\n    tx[idx==False,i] = mean'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tx = x.copy()\n",
    "for i in range(x.shape[1]):\n",
    "    idx = x[:,i] > -999\n",
    "    mean = np.mean(x[idx,i])\n",
    "    tx[idx==False,i] = mean'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = standardize(x)\n",
    "#add constant term\n",
    "tx = np.c_[np.ones((y.shape[0],1)), tx]\n",
    "tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = split_data(tx, y, 1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73704, 0.73708, 0.73374, 0.73668, 0.73508]\n",
      "[1.3924515051428645531, 1.3966437172033032885, 1.4076181071802289726, 1.3997037523526678248, 1.3968991804508836006]\n",
      "[0.47096, 0.61934, 0.4944, 0.52392, 0.595]\n",
      "[48756665.88962448199, 5848951170.4316515788, 4575.1970285782716905, 56656.796372831631633, 876.2883441005645191]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "max_iters = 50\n",
    "initial_w = np.zeros(x_train.shape[1])\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed=1)\n",
    "\n",
    "#gradients = [least_squares_GD(y_train, x_train, initial_w, max_iters,gamma),\n",
    "#             least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)]\n",
    "#gradients_names = [\"Gradient Descent\",\"Stochastic Gradient Descent\"]\n",
    "#print('-----Without standardization-----')\n",
    "#for i in range (len(gradients)):\n",
    "''''    w,loss = gradients[i]\n",
    "    print(\"{name}, w*={w}, loss={l}\\n\".format(name=gradients_names[i],w=w, l=loss))'''\n",
    "\n",
    "acc_GD = []\n",
    "total_loss_te_GD= []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc, loss_te_GD = cross_validation(y_train, x_train, k_indices, k, initial_w, 'least_squares_GD', max_iters, gamma)\n",
    "    acc_GD.append(np.mean(acc))\n",
    "    total_loss_te_GD.append(np.mean(loss_te_GD))\n",
    "    \n",
    "print(acc_GD)\n",
    "print(total_loss_te_GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73704, 0.73708, 0.73374, 0.73668, 0.73508]\n",
      "[1.3924515051428645531, 48756665.88962448199, 1.3966437172033032885, 5848951170.4316515788, 1.4076181071802289726, 4575.1970285782716905, 1.3997037523526678248, 56656.796372831631633, 1.3968991804508836006, 876.2883441005645191]\n",
      "[0.47096, 0.61934, 0.4944, 0.52392, 0.595]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(acc_GD)\n",
    "print(total_loss_te_GD)\n",
    "print(acc_SGD)\n",
    "print(total_loss_te_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "max_iters = 50\n",
    "initial_w = np.zeros(x_train.shape[1])\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed=1)\n",
    "\n",
    "acc_SGD = []\n",
    "total_loss_te_SGD= []\n",
    "for k in range(k_fold):    \n",
    "    acc, loss_te_SGD = cross_validation(y_train, x_train, k_indices, k, initial_w, 'least_squares_SGD', max_iters, gamma)\n",
    "    acc_SGD.append(np.mean(acc))\n",
    "    total_loss_te_SGD.append(np.mean(loss_te_SGD))\n",
    "\n",
    "print(acc_SGD)\n",
    "print(total_loss_te_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n",
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "lambda_ = 0.6\n",
    "'''\n",
    "regression =[least_squares(y_train, x_train),\n",
    "             ridge_regression(y_train, x_train,lambda_)]\n",
    "                  \n",
    "regression_names =  [\"Least Squares\", \"Ridge Regression\"]\n",
    "\n",
    "print('-----Without standardization-----')\n",
    "for i in range (len(regression)):\n",
    "    w,loss = regression[i]\n",
    "    print(\"{name}, w*={w}, loss={l}\\n\".format(name=regression_names[i],w=w, l=loss))\n",
    "'''\n",
    "    \n",
    "acc_LS = []\n",
    "total_loss_te_LS = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    \n",
    "    acc, loss_te_RR = cross_validation(y_train, x_train, k_indices, k, initial_w, 'ridge_regression', lambda_)\n",
    "    acc_RR.append(acc)\n",
    "    total_loss_te_RR.append(np.mean(loss_te_RR))\n",
    "\n",
    "print(acc_LS)\n",
    "print(total_loss_te_LS)\n",
    "\n",
    "for i in range (len(acc_RR)):\n",
    "    print(\"{K}, Loss={l}, Accuracy={a}\\n\".format(K=i,l=total_loss_te_RR[i], a=acc_RR[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n",
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n"
     ]
    }
   ],
   "source": [
    "print(acc_LS)\n",
    "print(total_loss_te_LS)\n",
    "print(acc_RR)\n",
    "print(total_loss_te_RR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rigde Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "acc_RR = []\n",
    "total_loss_te_RR = []\n",
    "for k in range(k_fold):\n",
    "    \n",
    "    acc, loss_te_RR = cross_validation(y_train, x_train, k_indices, k, initial_w, 'ridge_regression', lambda_)\n",
    "    acc_RR.append(np.mean(acc))\n",
    "    total_loss_te_RR.append(np.mean(loss_te_RR))\n",
    "\n",
    "print(acc_RR)\n",
    "print(total_loss_te_RR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Test accuracy : 0.731640\n",
      "1 - Test accuracy : 0.732560\n",
      "2 - Test accuracy : 0.728440\n",
      "3 - Test accuracy : 0.731620\n",
      "4 - Test accuracy : 0.728980\n",
      "\n",
      "Average test accuracy: 0.730648\n",
      "Variance test accuracy: 0.000003\n",
      "Min test accuracy: 0.728440\n",
      "Max test accuracy: 0.732560\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "gamma = 1e-6\n",
    "max_iters = 50\n",
    "lambda_ = 0.001\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed=1)\n",
    "\n",
    "'''\n",
    "logistic = [logistic_regression(y_train, x_train, initial_w, max_iters, gamma),\n",
    "            reg_logistic_regressions(y_train, x_train, lambda_, initial_w, max_iters, gamma)]\n",
    "logistic_names = ['Logistic Ridge Regression','Reg Logistic Ridge Regression']\n",
    "\n",
    "print('-----Without standardization-----')\n",
    "for i in range (len(logistic)):\n",
    "    w,loss = logistic[i]\n",
    "    print(\"{name}, w*={w}, loss={l}\\n\".format(name=logistic_names[i],w=w, l=loss))\n",
    "'''   \n",
    "acc_LR = []\n",
    "total_loss_te_LR = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc, loss_te_LR = cross_validation(y_train, x_train, k_indices, k, initial_w, 'logistic_regression', max_iters, gamma, lambda_)\n",
    "    acc_LR.append(acc)\n",
    "    total_loss_te_LR.append(loss_te_LR)\n",
    "\n",
    "for i in range(len(acc_LR)):\n",
    "    print(\"%d - Test accuracy : %f\" % (i, acc_LR[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(acc_LR))\n",
    "print(\"Variance test accuracy: %f\" % np.var(acc_LR))\n",
    "print(\"Min test accuracy: %f\" % np.min(acc_LR))\n",
    "print(\"Max test accuracy: %f\" % np.max(acc_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reg Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ML-2020/project1/scripts/helpers.py:54: RuntimeWarning: overflow encountered in exp\n",
      "  e_t =np.exp(-t)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-d83a2d324b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te_RLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reg_logistic_regression'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0macc_RLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_loss_te_RLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te_RLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML-2020/project1/scripts/helpers.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, initial_w, model_name, max_iters, gamma, lambda_)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'reg_logistic_regression'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML-2020/project1/scripts/implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "gamma = 0.01\n",
    "max_iters = 10\n",
    "lambda_ = 0.04\n",
    "\n",
    "\n",
    "acc_RLR = []\n",
    "total_loss_te_RLR = []\n",
    "for k in range(k_fold):\n",
    "    \n",
    "    acc, loss_te_RLR = cross_validation(y_tr, x_train, k_indices, k, initial_w, 'reg_logistic_regression',  max_iters, gamma, lambda_)\n",
    "    acc_RLR.append(acc)\n",
    "    total_loss_te_RLR.append(np.mean(loss_te_RLR))\n",
    "\n",
    "for i in range(len(acc_LR)):\n",
    "    print(\"%d - Mean loss : %f / Test accuracy : %f\" % (i, total_loss_te_RLR[i], acc_LR[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(acc_LR))\n",
    "print(\"Variance test accuracy: %f\" % np.var(acc_LR))\n",
    "print(\"Min test accuracy: %f\" % np.min(acc_LR))\n",
    "print(\"Max test accuracy: %f\" % np.max(acc_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
