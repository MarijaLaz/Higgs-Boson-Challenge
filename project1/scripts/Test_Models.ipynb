{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip csv \n",
    "import zipfile\n",
    "with zipfile.ZipFile('../data/train.csv.zip') as zip_ref:\n",
    "    zip_ref.extractall(r\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "#load data\n",
    "DATA_TRAIN_PATH = '../data/train.csv' #download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "print(y.shape, x.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = x.copy()\n",
    "for i in range(x.shape[1]):\n",
    "    idx = x[:,i] > -999\n",
    "    mean = np.mean(x[idx,i])\n",
    "    tx[idx==False,i] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = standardize(x)\n",
    "#add constant term\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), tx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = split_data(x, y, 1, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient and Stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'least_squares_GD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-19d47d10c58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtotal_loss_te_SGD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te_GD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'least_squares_GD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0macc_GD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtotal_loss_te_GD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_te_GD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML-2020/project1/scripts/helpers.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_indices, k, initial_w, model_name, max_iters, gamma, lambda_)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m#initial_w = np.zeros(x_tr.shape[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'least_squares_GD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'least_squares_SGD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'least_squares_GD' is not defined"
     ]
    }
   ],
   "source": [
    "gamma = 0.1\n",
    "max_iters = 50\n",
    "initial_w = np.zeros(x_train.shape[1])\n",
    "k_fold = 5\n",
    "k_indices = build_k_indices(y, k_fold, seed=1)\n",
    "\n",
    "#gradients = [least_squares_GD(y_train, x_train, initial_w, max_iters,gamma),\n",
    "#             least_squares_SGD(y_train, x_train, initial_w, max_iters, gamma)]\n",
    "#gradients_names = [\"Gradient Descent\",\"Stochastic Gradient Descent\"]\n",
    "#print('-----Without standardization-----')\n",
    "#for i in range (len(gradients)):\n",
    "''''    w,loss = gradients[i]\n",
    "    print(\"{name}, w*={w}, loss={l}\\n\".format(name=gradients_names[i],w=w, l=loss))'''\n",
    "    \n",
    "\n",
    "acc_GD = []\n",
    "total_loss_te_GD= []\n",
    "acc_SGD = []\n",
    "total_loss_te_SGD= []\n",
    "for k in range(k_fold):\n",
    "    acc, loss_te_GD = cross_validation(y_train, x_train, k_indices, k, initial_w, 'least_squares_GD', max_iters, gamma)\n",
    "    acc_GD.append(np.mean(acc))\n",
    "    total_loss_te_GD.append(np.mean(loss_te_GD))\n",
    "    \n",
    "    acc, loss_te_SGD = cross_validation(y_train, x_train, k_indices, k, initial_w, 'least_squares_SGD', max_iters, gamma)\n",
    "    acc_SGD.append(np.mean(acc))\n",
    "    total_loss_te_SGD.append(np.mean(loss_te_SGD))\n",
    "\n",
    "print(acc_GD)\n",
    "print(total_loss_te_GD)\n",
    "print(acc_SGD)\n",
    "print(total_loss_te_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73704, 0.73708, 0.73374, 0.73668, 0.73508]\n",
      "[1.3924515051428645531, 48756665.88962448199, 1.3966437172033032885, 5848951170.4316515788, 1.4076181071802289726, 4575.1970285782716905, 1.3997037523526678248, 56656.796372831631633, 1.3968991804508836006, 876.2883441005645191]\n",
      "[0.47096, 0.61934, 0.4944, 0.52392, 0.595]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(acc_GD)\n",
    "print(total_loss_te_GD)\n",
    "print(acc_SGD)\n",
    "print(total_loss_te_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares and Rigde Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7461, 0.74518, 0.7434, 0.74582, 0.74262]\n",
      "[1.3555913246193446571, 1.3582676360824257117, 1.3706891248972015524, 1.363524075555184476, 1.362251088917323285]\n",
      "[0.7461, 0.74518, 0.7434, 0.74582, 0.74262]\n",
      "[1.3555913246193446571, 1.3582676360824257117, 1.3706891248972015524, 1.363524075555184476, 1.362251088917323285]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "initial_w = np.zeros(31)\n",
    "lambda_ = 0.00001\n",
    "'''\n",
    "regression =[least_squares(y_train, x_train),\n",
    "             ridge_regression(y_train, x_train,lambda_)]\n",
    "                  \n",
    "regression_names =  [\"Least Squares\", \"Ridge Regression\"]\n",
    "\n",
    "print('-----Without standardization-----')\n",
    "for i in range (len(regression)):\n",
    "    w,loss = regression[i]\n",
    "    print(\"{name}, w*={w}, loss={l}\\n\".format(name=regression_names[i],w=w, l=loss))\n",
    "'''\n",
    "    \n",
    "acc_LS = []\n",
    "total_loss_te_LS = []\n",
    "acc_RR = []\n",
    "total_loss_te_RR = []\n",
    "for k in range(k_fold):\n",
    "    acc, loss_te_LS = cross_validation(y_train, x_train, k_indices, k, initial_w, 'least_squares', lambda_)\n",
    "    acc_LS.append(np.mean(acc))\n",
    "    total_loss_te_LS.append(np.mean(loss_te_LS))\n",
    "    \n",
    "    acc, loss_te_RR = cross_validation(y_train, x_train, k_indices, k, initial_w, 'ridge_regression', lambda_)\n",
    "    acc_RR.append(np.mean(acc))\n",
    "    total_loss_te_RR.append(np.mean(loss_te_RR))\n",
    "\n",
    "print(acc_LS)\n",
    "print(total_loss_te_LS)\n",
    "print(acc_RR)\n",
    "print(total_loss_te_RR)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n",
      "[0.74654, 0.74512, 0.74376, 0.74442, 0.74374]\n",
      "[1.3525886178173593112, 1.3545276600558762962, 1.3668025050377936305, 1.3620093289471031241, 1.355622976245170133]\n"
     ]
    }
   ],
   "source": [
    "print(acc_LS)\n",
    "print(total_loss_te_LS)\n",
    "print(acc_RR)\n",
    "print(total_loss_te_RR)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression and Reg Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Without standardization-----\n",
      "Logistic Ridge Regression, w*=[-1.10725936e+01  8.41469761e-02 -3.60034846e+00 -1.00180863e-01\n",
      "  1.34780359e+00  1.34314996e+00  1.24489798e+00 -1.15490057e+00\n",
      "  6.77071689e-01 -3.53528957e-01  8.75728625e-01 -1.96086787e+00\n",
      "  2.54229826e+00  1.35989188e+00  2.02666432e+00 -1.06067284e-02\n",
      " -5.25302770e-02 -4.38613373e-01  1.31998910e-02  4.22363295e-02\n",
      " -2.51981090e-01  6.96128266e-02  7.43541349e-01  8.24386799e-01\n",
      "  3.00275454e-01  4.22169233e-05  1.79620915e-04 -3.88428881e-01\n",
      "  1.13832281e-02 -2.71927538e-02  6.69111205e-01], loss=nan\n",
      "\n",
      "Reg Logistic Ridge Regression, w*=[-1.10731186e+01  8.35844548e-02 -3.60102072e+00 -1.00770113e-01\n",
      "  1.34735234e+00  1.34253135e+00  1.24429954e+00 -1.15554503e+00\n",
      "  6.76363933e-01 -3.54053977e-01  8.75313981e-01 -1.96147333e+00\n",
      "  2.54175795e+00  1.35930844e+00  2.02610867e+00 -1.11926643e-02\n",
      " -5.31476897e-02 -4.39136504e-01  1.26137242e-02  4.16239858e-02\n",
      " -2.52471097e-01  6.90056168e-02  7.43110012e-01  8.23897864e-01\n",
      "  2.99825105e-01 -5.53520078e-04 -4.42024442e-04 -3.88921735e-01\n",
      "  1.07896842e-02 -2.77913329e-02  6.68693852e-01], loss=nan\n",
      "\n",
      "[0.702, 0.7025, 0.69822, 0.70416, 0.70172]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[0.702, 0.70248, 0.69824, 0.70414, 0.70178]\n",
      "[nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(31)\n",
    "gamma = 1e-6\n",
    "max_iters = 50\n",
    "lambda_ = 1\n",
    "'''\n",
    "logistic = [logistic_regression(y_train, x_train, initial_w, max_iters, gamma),\n",
    "            reg_logistic_regressions(y_train, x_train, lambda_, initial_w, max_iters, gamma)]\n",
    "logistic_names = ['Logistic Ridge Regression','Reg Logistic Ridge Regression']\n",
    "\n",
    "print('-----Without standardization-----')\n",
    "for i in range (len(logistic)):\n",
    "    w,loss = logistic[i]\n",
    "    print(\"{name}, w*={w}, loss={l}\\n\".format(name=logistic_names[i],w=w, l=loss))\n",
    "'''   \n",
    "    \n",
    "acc_LR = []\n",
    "total_loss_te_LR = []\n",
    "acc_RLR = []\n",
    "total_loss_te_RLR = []\n",
    "for k in range(k_fold):\n",
    "    acc, loss_te_LR = cross_validation(y_train, x_train, k_indices, k, initial_w, 'logistic_regression', max_iters, gamma, lambda_)\n",
    "    acc_LR.append(np.mean(acc))\n",
    "    total_loss_te_LR.append(np.mean(loss_te_LR))\n",
    "    \n",
    "    acc, loss_te_RLR = cross_validation(y_train, x_train, k_indices, k, initial_w, 'reg_logistic_regressions',  max_iters, gamma, lambda_)\n",
    "    acc_RLR.append(np.mean(acc))\n",
    "    total_loss_te_RLR.append(np.mean(loss_te_RLR))\n",
    "\n",
    "print(acc_LR)\n",
    "print(total_loss_te_LR)\n",
    "print(acc_RLR)\n",
    "print(total_loss_te_RLR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68926, 0.69136, 0.6854, 0.6904, 0.68868]\n",
      "[nan, inf, inf, nan, nan]\n",
      "[0.68932, 0.69134, 0.68534, 0.69042, 0.68872]\n",
      "[nan, inf, inf, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(acc_LR)\n",
    "print(total_loss_te_LR)\n",
    "print(acc_RLR)\n",
    "print(total_loss_te_RLR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
